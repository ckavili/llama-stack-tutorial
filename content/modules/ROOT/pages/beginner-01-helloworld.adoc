= Hello Llama Stack!
:page-layout: lab
:experimental:

== Goal

In this module, you will launch the Llama Stack server using a model served by Ollama, and make your first LLM request using the `llama-stack-client` CLI.

== Prerequisites

* Ollama installed on your local machine (https://ollama.com/download)
* Docker or Podman installed
* Python 3.10+ (only for installing the CLI)

== Step 1: Start the Ollama Server

Open a terminal and run the Ollama background service:

[source,sh,role=execute]
----
ollama serve
----

Leave this terminal open and running.

== Step 2: Load the Model

Llama Stack does not dynamically load models from Ollama. You need to preload and keep the model in memory.

Open a second terminal and run:

[source,sh,role=execute]
----
ollama run llama3.2:3b-instruct-fp16 --keepalive 60m
----

To confirm the model is running and in memory:

[source,sh,role=execute]
----
ollama ps
----

You should see `llama3.2:3b-instruct-fp16` listed.

== Step 3: Start the Llama Stack Server

Open a third terminal.

First, export the necessary environment variables:

[source,sh,role=execute]
----
export LLAMA_STACK_MODEL="meta-llama/Llama-3.2-3B-Instruct"
export INFERENCE_MODEL="meta-llama/Llama-3.2-3B-Instruct"
export LLAMA_STACK_PORT=8321
export LLAMA_STACK_SERVER=http://localhost:$LLAMA_STACK_PORT
----

Now run the Llama Stack container using Podman:

[source,sh,role=execute]
----
podman run -it \
  -p $LLAMA_STACK_PORT:$LLAMA_STACK_PORT \
  -v ~/.llama:/root/.llama \
  llamastack/distribution-ollama \
  --port $LLAMA_STACK_PORT \
  --env INFERENCE_MODEL=$LLAMA_STACK_MODEL \
  --env OLLAMA_URL=http://host.docker.internal:11434
----

The Llama Stack server will start and listen on port `8321`.

== Step 4: Hello World with the CLI

Create and activate a Python virtual environment:

[source,sh,role=execute]
----
python -m venv llama_env
source llama_env/bin/activate
----

Install the Llama Stack client CLI:

[source,sh,role=execute]
----
pip install llama-stack-client
----

Configure the CLI to point to your running server:

[source,sh,role=execute]
----
llama-stack-client configure --endpoint http://localhost:8321
----

Now, send your first prompt using the CLI:

[source,sh,role=execute]
----
llama-stack-client \
  inference chat-completion \
  --message "hello, what model are you?" \
  --model-id "meta-llama/Llama-3.2-3B-Instruct"
----

== Sample Output

[source,txt]
----
ChatCompletionResponse(completion_message=CompletionMessage(
    content="Hello! I'm a Meta LLaMA 3.2 3B Instruct model. How can I assist you today?",
    role='assistant',
    stop_reason='end_of_turn',
    tool_calls=[]),
    logprobs=None,
    metrics=[
        Metric(metric='prompt_tokens', value=12.0, unit=None),
        Metric(metric='completion_tokens', value=24.0, unit=None),
        Metric(metric='total_tokens', value=36.0, unit=None)
    ]
)
----

This shows a typical structured response from the model via the CLI. You may see different content depending on model version and configuration.

== Summary

You have:

* Started Ollama and preloaded the Llama 3.2 3B model
* Launched the Llama Stack server in a container
* Sent a basic prompt using the Llama Stack CLI

You are now ready to build more advanced Llama Stack applications using either the CLI or Python SDK!
