= Hello Llama Stack!
:page-layout: lab
:experimental:

== Goal

In this module, you will launch the Llama Stack server using a model served by Ollama, and make your first LLM request using the Llama Stack Python SDK.

== Prerequisites

* Ollama installed on your local machine (https://ollama.com/download)
* Docker or Podman installed
* Python 3.10+

== Step 1: Start the Ollama Server

Open a terminal and run the Ollama background service:

[source,sh,role=execute]
----
ollama serve
----

Leave this terminal open and running.

== Step 2: Load the Model with Keepalive

Llama Stack does not dynamically load models from Ollama. You need to preload and keep the model in memory.

Open a second terminal and run:

[source,sh,role=execute]
----
ollama run llama3.2:3b-instruct-fp16 --keepalive 60m
----

To confirm the model is running and in memory:

[source,sh,role=execute]
----
ollama ps
----

You should see `llama3.2:3b-instruct-fp16` listed.

== Step 3: Start the Llama Stack Server

Open a third terminal.

First, export the necessary environment variables:

[source,sh,role=execute]
----
export LLAMA_STACK_MODEL="meta-llama/Llama-3.2-3B-Instruct"
export INFERENCE_MODEL="meta-llama/Llama-3.2-3B-Instruct"
export LLAMA_STACK_PORT=8321
export LLAMA_STACK_SERVER=http://localhost:$LLAMA_STACK_PORT
----

Now run the Llama Stack container using Podman:

[source,sh,role=execute]
----
podman run -it \
  -p $LLAMA_STACK_PORT:$LLAMA_STACK_PORT \
  -v ~/.llama:/root/.llama \
  llamastack/distribution-ollama \
  --port $LLAMA_STACK_PORT \
  --env INFERENCE_MODEL=$LLAMA_STACK_MODEL \
  --env OLLAMA_URL=http://host.docker.internal:11434
----

The Llama Stack server will start and listen on port `8321`.

== Step 4: Hello World with the Python SDK

Create and activate a Python virtual environment:

[source,sh,role=execute]
----
python -m venv llama_env
source llama_env/bin/activate
----

Install the Llama Stack client SDK:

[source,sh,role=execute]
----
pip install llama-stack-client
----

Create a Python script named `hello_llama.py`:

[source,python,role=execute]
----
cat << 'EOF' > hello_llama.py
from llama_stack_client import LlamaStackClient

client = LlamaStackClient(base_url="http://localhost:8321")

response = client.inference.chat_completion(
    messages=[{"role": "user", "content": "Hello, Llama Stack!"}],
    model_id="meta-llama/Llama-3.2-3B-Instruct"
)

print("Model Response:")
print(response.completion_message.content)

EOF

----

Run the script:

[source,sh,role=execute]
----
python hello_llama.py
----

You should get a simple response similar to:

[source,txt]
----
Model Response:
Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?
----

This shows a typical response from the model. You can expect variations depending on the version, temperature settings, or prompt phrasing.

== Summary

You have:

* Started Ollama and preloaded the Llama 3.2 3B model
* Launched the Llama Stack server in a container
* Sent a basic prompt using the Python SDK

You are now ready to build more advanced Llama Stack applications!
