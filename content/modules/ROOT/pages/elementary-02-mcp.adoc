= Integrating a Simple MCP Weather Server with Llama Stack
:page-layout: lab
:experimental:

== Goal

In this module, you'll create a simple calculator server using the MCP (Model Context Protocol) Python SDK and integrate it with Llama Stack. This exercise demonstrates how to expose custom logic as tool-like services that can be invoked by agents inside the Llama Stack ecosystem. By the end of this module, you'll understand how to turn any Python function into a callable "tool" via MCP and how to wire it into a fully agentic workflow.

== Prerequisites

* Llama Stack server running (see: xref:beginner-01-helloworld.adoc[Llama-stack Helloworld])
* Python 3.10+ installed on your local machine
* `npx` installed on your machine (tested with version 10.8+)

== Step 1: Run a simple calculator MCP server

We'll use podman to run a simple MPC calculator server.

[source,sh,role=execute]
----
podman run -e SUPERGATEWAY_PORT=8020 -p 8020:8020 quay.io/rh-aiservices-bu/mcp-simple-calc:latest
----

This runs your calculator server and makes it available at `http://localhost:8020`.

== Step 2: Register the MCP Server in Llama Stack

To allow Llama Stack to call this tool, you must register it as a toolgroup:

[source,sh,role=execute]
----
curl -X POST -H "Content-Type: application/json" \
--data \
'{ "provider_id" : "model-context-protocol", "toolgroup_id" : "mcp::simple_calc", "mcp_endpoint" : { "uri" : "http://localhost:8020/sse"}}' \
 http://localhost:8321/v1/toolgroups 
----

This step tells Llama Stack that your local calculator server is available and that it supports tools grouped under the ID `mcp::simple_calc`.

== Step 3: Query the simple_calc via Llama Stack

Now, let's test the integration by writing a Python script that uses the Llama Stack client SDK to send a prompt to the registered toolgroup.

Run this command to create a file called test_mcp.py
[source,sh,role=execute]
----
cat << 'EOF' >  test_mcp.py
from llama_stack_client.lib.agents.event_logger import EventLogger
base_url = "http://localhost:8321"

from llama_stack_client import LlamaStackClient

client = LlamaStackClient(
    base_url=base_url
)
model = "meta-llama/Llama-3.2-3B-Instruct"

# System prompt configures the assistant behavior
sys_prompt1 = """You are a helpful assistant. Use tools to answer. When you use a tool always respond with a summary of the result."""

from llama_stack_client import Agent

# Create an agent that will use the simple_calc tools to respond to user queries
agent = Agent(
    client,
    model=model,
    instructions=sys_prompt1,
    tools=["mcp::simple_calc"],  # Use the toolgroup we registered earlier
    tool_config={"tool_choice": "auto"},  # Automatically choose tools based on prompt
)

user_prompts = ["What is the sum of 1 + 2?"]
session_id = agent.create_session(session_name="calc_demo")

for prompt in user_prompts:
    turn_response = agent.create_turn(
        messages=[
            {
                "role": "user",
                "content": prompt
            }
        ],
        session_id=session_id,
        stream=True,
    )
    for log in EventLogger().log(turn_response):
        log.print()
EOF
----

This script sets up a simple LLM agent with access to our calculator toolgroup. It sends a prompt to the model, which detects the need to use a tool, Llama Stack calls the tool and returns the answer to the model, the model returns a human-readable answer.

== Step 5: Run the Test

Install the Llama Stack Client sdk

[source,sh,role=execute]
----
pip install llama-stack-client==0.2.2
----

Run the Python test script:

[source,sh,role=execute]
----
python test_mcp.py
----

Expected output:

[source,txt]
----
inference> [add(a=1, b=2)]
tool_execution> Tool:add Args:{'a': 1.0, 'b': 2.0}
tool_execution> Tool:add Response:{"type":"text","text":"3","annotations":null}
inference> The result of the operation is 3.
----

This confirms that the agent successfully routed the request to your MCP server and returned the result from the `add()` tool.

== Summary

In this module, you:

* Deployed a simple calculator MCP server using podman
* Registered it as a `toolgroup` in Llama Stack
* Tested the connection by querying the server through an LLM agent in a Python script

This pattern can be extended to wrap any business logic or microservice in a tool interface, enabling LLMs to use your logic like plug-and-play models. Itâ€™s a powerful way to make custom tools available to AI agents.
