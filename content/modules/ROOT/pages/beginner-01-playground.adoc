= Llama-Stack Playground
:page-layout: lab
:experimental:

== Goal

This module introduces the **Llama-Stack Dashboard**, a user-friendly interface designed to demonstrate the core capabilities of Llama Stack in an interactive environment.
The dashboard allows you to inspect API providers and resources exposed by Llama Stack.

== Prerequisites

Before starting, ensure you have the following:

* A running Llama Stack server (see: xref:beginner-01-helloworld.adoc[Llama-Stack Hello World])
* Python 3.10 or newer (to install and run the CLI tools)

== Step 1: Deploy the Llama-Stack Dashboard

You can deploy the dashboard using the https://llama-stack.readthedocs.io/en/latest/playground/index.html[source code] or run the pre-built container image from Quay.io:

[source,sh,role=execute]
----
podman run -p 8501:8501 \
  -e LLAMA_STACK_ENDPOINT=http://host.containers.internal:8321 \
  quay.io/rh-aiservices-bu/llama-stack-playground:0.1.9
----

Once the container is running, the Llama-Stack Dashboard will be available at `http://localhost:8501`.

== Step 2: Interact with the Dashboard Chat

The Llama-Stack Dashboard is built with Streamlit and includes a chat interface for interacting with the Llama Stack server.

Open your browser and navigate to `http://localhost:8501`. Youâ€™ll see a chat interface where you can enter prompts and receive responses from the deployed LLM model.

image::../assets/images/llama-stack-dashboard.png[Llama-Stack Dashboard, width=800]

== Step 3: Explore the Dashboard

The dashboard also allows you to explore available resources, such as models, vector databases, and shields.

To browse these:

* Click the "Resources" tab to view all registered components in your Llama Stack setup.

image::../assets/images/llama-stack-dashboard3.png[Resources, width=800]

* Click the "API Providers" tab to inspect configured model and embedding providers.

image::../assets/images/llama-stack-dashboard2.png[API Providers, width=800]

== Summary

In this module, you:

* Deployed and launched the Llama-Stack Dashboard
* Interacted with the LLM model through the chat interface
* Explored available models, providers, and other resources

Next, continue with xref:beginner-01-python-programming.adoc[Llama-Stack Python Programming] to write and run your first Python program using Llama Stack.
